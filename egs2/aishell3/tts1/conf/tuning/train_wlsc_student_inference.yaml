# This configuration is for ESPnet2 to train Conformer-based
# FastSpeech2 with GST + X-vector. It requires 4 GPU with 32 GB
# memory and it takes ~3 days to finish the training on V100.

# Compared to the original FastSpeech2 paper, we use token
# averaged pitch and energy as the same as FastPitch.
# And we do not use quantized pitch and energy.

# For FastSpeech2, we need to extract pitch and energy.
# Therefore, we assume that feats_type=raw in using this
# configuration. Please be careful.

##########################################################
#                  TTS MODEL SETTING                     #
##########################################################
tts: wlsc      # model architecture
tts_conf:             # keyword arguments for the selected model
    # spk_embed_dim: 128
    # phone_embed_dim: 128
    # word_seq_embed_dim: 128
    # word_seq_proj_dim: 128
    # style_token_dim: 128
    # word_seq_encoder_dropout: 0.0
    # encoder_adim: 128
    # encoder_aheads: 2
    encoder_numblocks: 1
    # encoder_dropout_rate: 0.0
    # encoder_positional_dropout_rate: 0.0
    # encoder_attention_dropout_rate: 0.0
    # encoder_conv_kernel_size: 5
    # style_encoder_aheads: 2
    # decoder_adim: 512
    # decoder_aheads: 2
    decoder_numblocks: 2
    # decoder_dropout_rate: 0.0
    # decoder_positional_dropout_rate: 0.0
    # decoder_attention_dropout_rate: 0.0
    # decoder_conv_kernel_size: 5
    # prior_layers: 1
    # duration_predictor_layers: 1
    # duration_predictor_chans: 128
    # duration_predictor_kernel_size: 5
    # duration_predictor_dropout: 0.0
    # range_predictor_layers: 1
    # range_predictor_chans: 128
    # range_predictor_kernel_size: 5
    # range_predictor_dropout: 0.0
    teacher_conf: ./exp/tts_train_wlsc_wlsc_phn_none/config.yaml
    teacher_ckpt: ./exp/tts_train_wlsc_wlsc_phn_none/latest.pth


##########################################################
#            OPTIMIZER & SCHEDULER SETTING               #
##########################################################
optim: adam            # optimizer type
optim_conf:            # keyword arguments for selected optimizer
    lr: 1            # learning rate
scheduler: noamlr      # scheduler type
scheduler_conf:        # keyword arguments for selected scheduler
    model_size: 1536    # model size, a.k.a., attention dimension
    warmup_steps: 2000 # the number of warmup steps

##########################################################
#                OTHER TRAINING SETTING                  #
##########################################################
num_iters_per_epoch: 500  # number of iterations per epoch
max_epoch: 1000            # number of epochs
grad_clip: 1.0            # gradient clipping norm
grad_noise: false         # whether to use gradient noise injection
accum_grad: 1             # gradient accumulation
# batch_bins: 1800000      # batch bins (feats_type=raw)
batch_bins: 17500      # batch bins (feats_type=raw)
batch_type: numel         # how to make batch
sort_in_batch: descending # how to sort data in making batch
sort_batch: descending    # how to sort created batches
num_workers: 1            # number of workers of data loader
train_dtype: float32      # dtype in training
log_interval: null        # log interval in iterations
keep_nbest_models: 5      # number of models to keep
num_att_plot: 3           # number of attention figures to be saved in every check
seed: 0                   # random seed number
best_model_criterion:     # criterion to save the best models
-   - valid
    - loss
    - min
-   - train
    - loss
    - min
